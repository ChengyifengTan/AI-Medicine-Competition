
# æ•°æ®æ„å»ºä»£ç è¯´æ˜

è¯¥ä»£ç ç”¨äºæ„å»ºé€‚é… MedM-VL å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæ ¼å¼çš„æ•°æ®é›†ã€‚

### 1. æ¨èé¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ script/                  
    â”œâ”€â”€ build_dataset.py     # æ•°æ®é›†æ„å»ºä»£ç 
    â”œâ”€â”€ build.sh             # æ•°æ®é›†æ„å»ºè„šæœ¬
    â”œâ”€â”€ split.py             # åˆ‡åˆ†å¼€å‘é›†çš„æ ·ä¾‹ä»£ç 
    â”œâ”€â”€ templates.json       # å„ä»»åŠ¡çš„promptæ¨¡æ¿
    â”œâ”€â”€ test_predict.json    # æ ·ä¾‹çš„æäº¤æ ¼å¼ï¼ŒåŒæ—¶ä½œä¸ºæµ‹è¯•é›†æ ‡ç­¾ï¼ˆæ ‡ç­¾éƒ½æ˜¯0ï¼‰
    â””â”€â”€ evaluate.py          # ç»“æœè¯„ä¼°ä»£ç             
â””â”€â”€ data/ 
    â”œâ”€â”€ bounding_box_train.json  # å…¨éƒ¨æ•°æ®çš„bounding boxä¿¡æ¯
    â”œâ”€â”€ train.json           # å…¨éƒ¨æ•°æ®çš„æ ‡ç­¾
    â”œâ”€â”€ train_label.json     # è®­ç»ƒé›†æ ‡ç­¾ï¼ˆéå®šä½ä»»åŠ¡ï¼‰
    â”œâ”€â”€ dev_label.json       # å¼€å‘é›†æ ‡ç­¾ï¼ˆéå®šä½ä»»åŠ¡ï¼‰
    â”œâ”€â”€ train_box.json       # è®­ç»ƒé›†å®šä½æ¡†ï¼ˆå®šä½ä»»åŠ¡ï¼‰
    â”œâ”€â”€ dev_box.json         # å¼€å‘é›†å®šä½æ¡†ï¼ˆå®šä½ä»»åŠ¡ï¼‰
    â”œâ”€â”€ dataset/             # è¾“å‡ºæ•°æ®ç›®å½•
    â””â”€â”€ mri_images/          # å›¾ç‰‡æ•°æ®æ€»ç›®å½•
        â”œâ”€â”€ train/            
    	â””â”€â”€ test/          
```

æ¨èçš„é¡¹ç›®ä»£ç åº“ç»“æ„å¦‚ä¸Šï¼Œå…¶ä¸­train_label.jsonã€dev_label.jsonä¸ºåˆ‡åˆ†train.jsonå¾—åˆ°çš„è®­ç»ƒé›†å’Œå¼€å‘é›†ï¼Œtrain_box.jsonã€dev_box.jsonä¸ºåˆ‡åˆ†bounding_box_train.jsonçš„å®šä½ä¿¡æ¯è®­ç»ƒé›†å’Œå¼€å‘é›†ã€‚

### 2. build.shå‚æ•°ä¿¡æ¯

| å‚æ•°                  | ç±»å‹      | é»˜è®¤å€¼                                     | æè¿°                                                         |
| --------------------- | --------- | ------------------------------------------ | ------------------------------------------------------------ |
| `--task_type`         | list[str] | `["qd","sl","zjppt","zyzg","positioning"]` | è¦æ„å»ºçš„ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬ï¼š`qd`ã€`sl`ã€`zjppt`ã€`zyzg`ã€`positioning` |
| `--train_images_path` | str       | `/data/mri_images/train`                   | è®­ç»ƒå›¾åƒçš„æ ¹ç›®å½•                                             |
| `--test_images_path`  | str       | `/data/mri_images/train`                   | æµ‹è¯•å›¾åƒçš„æ ¹ç›®å½•                                             |
| `--train_label_path`  | str       | `/data/train_label.json`                   | éå®šä½ä»»åŠ¡çš„è®­ç»ƒæ ‡ç­¾æ–‡ä»¶è·¯å¾„                                 |
| `--test_label_path`   | str       | `/data/dev_label.json`                     | éå®šä½ä»»åŠ¡çš„æµ‹è¯•æ ‡ç­¾æ–‡ä»¶è·¯å¾„                                 |
| `--train_box`         | str       | `/data/train_box.json`                     | å®šä½ä»»åŠ¡çš„è®­ç»ƒæ¡†æ ‡æ³¨æ–‡ä»¶è·¯å¾„                                 |
| `--test_box`          | str       | `/data/dev_box.json`                       | å®šä½ä»»åŠ¡çš„æµ‹è¯•æ¡†æ ‡æ³¨æ–‡ä»¶è·¯å¾„                                 |
| `--output_folder`     | str       | `/data/dataset`                            | è¾“å‡ºç”Ÿæˆçš„ JSON æ•°æ®é›†çš„ç›®å½•                                 |
| `--sag_image`         | list[int] | `[6]`                                      | æ‰€ä½¿ç”¨çš„çŸ¢çŠ¶é¢å›¾åƒç¼–å·ï¼Œ[5ï¼Œ6]ä¼šåŒæ—¶ç»™äºˆæ¨¡å‹ä¸¤å¼ çŸ¢çŠ¶ä½å›¾åƒ   |
| `--sag_type`          | str       | `""`                                       | è®¾ç½®ä¸º `seperate` æ—¶ï¼Œä¼šä¸ºæ¯ä¸ªçŸ¢çŠ¶åˆ‡ç‰‡å•ç‹¬æ„å»ºæ•°æ®ï¼Œå¦‚sag_imageä¸º[5,6,7]æ—¶ç›¸å½“äºå•ç‹¬æ‰§è¡Œ[5]ã€[6]ã€[7]ï¼ˆä¸ä¼šå½±å“å®šä½ä»»åŠ¡ï¼‰ |
| `--suffix`            | str       | `""`                                       | è¾“å‡ºæ–‡ä»¶åçš„åç¼€                                             |
| `--type_dataset`      | str       | `"both"`                                   | å¯é€‰ `train`ã€`dev`ã€`test` æˆ– `both`ï¼ŒæŒ‡å®šæ„å»ºæ•°æ®é›†çš„å½¢å¼ï¼Œbothä»£è¡¨`train`å’Œ`dev` |

### 3. split.pyåˆ‡åˆ†å¼€å‘é›†ä»£ç 

è¯¥ä»£ç ç®€å•çš„åˆ’åˆ†äº†è®­ç»ƒé›†å’Œå¼€å‘é›†ï¼Œå¯ä»¥å‚è€ƒä¿®æ”¹

### 4. éœ€è¦æäº¤çš„å†…å®¹

ä¸è¦æäº¤æ„å»ºçš„å¼€å‘é›†çš„ç­”æ¡ˆï¼

å®é™…çš„æµ‹è¯•é›†æ•°æ®æ˜¯mri_images/testä¸­çš„å›¾åƒæ•°æ®ï¼Œæ ‡ç­¾ä¿¡æ¯åœ¨data/test_predict.jsonä¸­ï¼Œè¯¥æ–‡ä»¶åŒæ—¶ä¸ºæäº¤æ–‡ä»¶çš„æ ·ä¾‹ï¼Œå…¶ä¸­æ‰€æœ‰æ ‡ç­¾éƒ½è¢«è®¾ç½®ä¸ºäº†0ã€‚

### 5. evaluate.pyè¯„ä¼°è„šæœ¬

è°ƒç”¨ç¤ºä¾‹å¦‚ä¸‹ï¼Œéœ€è¦æ›¿æ¢æ ‡ç­¾ä½ç½®å’Œé¢„æµ‹ç”Ÿæˆçš„ç­”æ¡ˆä½ç½®

æ–‡ä»¶æ ¼å¼åº”å’Œdata/test_predict.jsonä¸€è‡´

python evaluate.py --label_path "../data/test_label.json" --answer_path "./output/answer.json"

### 6. å‚è€ƒè®­ç»ƒæ•°æ®æ„å»ºè„šæœ¬

ï¼ˆ1ï¼‰åˆ‡åˆ†æ•°æ®ï¼Œå¾—åˆ°è®­ç»ƒé›†å’Œå¼€å‘é›†æ•°æ®

è¿è¡Œpython split.py

ï¼ˆ2ï¼‰æ„å»ºè®­ç»ƒé›†å’Œå¼€å‘é›†

æ›²åº¦å’Œé¡ºåˆ—ä»»åŠ¡æ•°æ®è¾ƒå°‘ï¼Œæ¨èä½¿ç”¨çŸ¢çŠ¶ä½5ã€6ã€7ä¸‰å¼ å›¾ç‰‡åˆ†åˆ«æ„å»ºè®­ç»ƒæ•°æ®ï¼Œå®šä½æ•°æ®åŒæ ·æ¨èä½¿ç”¨æ›´å¤šæ•°æ®

```
python build_dataset.py \
    --task_type '["qd","sl","positioning"]' \
    --train_images_path "../data/mri_images/train" \
    --test_images_path "../data/mri_images/train" \
    --train_label_path "../data/train_label.json" \
    --test_label_path "../data/dev_label.json" \
    --train_box "../data/train_box.json" \
    --test_box "../data/dev_box.json" \
    --output_folder "../data/dataset" \
    --sag_image "[5,6,7]" \
    --sag_type "seperate" \
    --suffix "" \
    --type_dataset "train"
```

æ¤é—´ç›˜è†¨çªå’Œä¸­å¤®æ¤ç®¡ä»»åŠ¡æ•°æ®è¾ƒå¤šï¼Œå¯ä»¥åªç”¨æ­£ä¸­ä¸€å¼ å›¾ç‰‡æ„å»ºæ•°æ®

```
python build_dataset.py \
    --task_type '["zjppt","zyzg"]' \
    --train_images_path "../data/mri_images/train" \
    --test_images_path "../data/mri_images/train" \
    --train_label_path "../data/train_label.json" \
    --test_label_path "../data/dev_label.json" \
    --train_box "../data/train_box.json" \
    --test_box "../data/dev_box.json" \
    --output_folder "../data/dataset" \
    --sag_image "[6]" \
    --sag_type "" \
    --suffix "" \
    --type_dataset "train"
```

ï¼ˆ3ï¼‰æ„å»ºå¼€å‘é›†

```
python build_dataset.py \
    --task_type '["qd","sl","zjppt","zyzg"]' \
    --train_images_path "../data/mri_images/train" \
    --test_images_path "../data/mri_images/train" \
    --train_label_path "../data/train_label.json" \
    --test_label_path "../data/dev_label.json" \
    --train_box "../data/train_box.json" \
    --test_box "../data/dev_box.json" \
    --output_folder "../data/dataset" \
    --sag_image "[6]" \
    --sag_type "" \
    --suffix "" \
    --type_dataset "dev"
```

ï¼ˆ4ï¼‰æ„å»ºæµ‹è¯•é›†

è¿™é‡Œç®€å•ä½¿ç”¨çŸ¢çŠ¶ä½æ­£ä¸­ä¸€å¼ å›¾ç‰‡

```
python build_dataset.py \
    --task_type '["qd","sl","zjppt","zyzg"]' \
    --train_images_path "../data/mri_images/train" \
    --test_images_path "../data/mri_images/test" \
    --train_label_path "../data/train_label.json" \
    --test_label_path "../data/test_predict.json" \
    --train_box "../data/train_box.json" \
    --test_box "" \
    --output_folder "../data/dataset" \
    --sag_image "[6]" \
    --sag_type "" \
    --suffix "" \
    --type_dataset "test"
```


# MedM-VL: What Makes a Good Medical LVLM?

[![arXiv](https://img.shields.io/badge/Arxiv-2504.04323-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2504.04323) [![hf_space](https://img.shields.io/badge/ğŸ¤—-%20Open%20In%20HF-blue.svg)](https://huggingface.co/collections/shiym2000/medm-vl-67f739e50d344d712eb7b010) [![License](https://img.shields.io/badge/License-Apache%202.0-yellow)](./LICENSE)

![architecture](./assets/architecture.png)

MedM-VL is a **modular**, LLaVA-based codebase for medical LVLMs, supporting flexible customization of encoders, connectors, and LLMs.

MedM-VL focuses on **small-scale** medical LVLMs, designed for **direct deployment** in real-world medical scenarios or **efficient fine-tuning** on downstream tasks.


## :newspaper: News

+ **[2025.04.10]**: The model weights (v1.0) have been uploaded to Hugging Face.
  + [shiym2000/MedM-VL-2D-3B-en Â· Hugging Face](https://huggingface.co/shiym2000/MedM-VL-2D-3B-en)
  + [shiym2000/MedM-VL-CT-Chest-3B-en Â· Hugging Face](https://huggingface.co/shiym2000/MedM-VL-CT-Chest-3B-en)
  + [shiym2000/MedM-CLIP-CT Â· Hugging Face](https://huggingface.co/shiym2000/MedM-CLIP-CT)
+ **[2025.04.06]**: The technical report has been released on arXiv.
  + [\[2504.04323\] MedM-VL: What Makes a Good Medical LVLM?](https://arxiv.org/abs/2504.04323)
+ **[2024.12.19]**: The complete code has been released on GitHub.


## :sparkles: Features

MedM-VL (v1.0: single image input, more details on Hugging Face)
+ [shiym2000/MedM-VL-2D-3B-en Â· Hugging Face](https://huggingface.co/shiym2000/MedM-VL-2D-3B-en): Trained on **2D** medical images and **English** medical texts.
+ [shiym2000/MedM-VL-CT-Chest-3B-en Â· Hugging Face](https://huggingface.co/shiym2000/MedM-VL-CT-Chest-3B-en): Trained on **3D** chest CT volumes and **English** medical texts.


## :package: Installation

``` bash
# 1. clone and navigate
git clone https://github.com/MSIIP/MedM-VL.git
cd MedM-VL

# 2. create a conda environment, activate it and install packages
conda create -n medm python=3.10
conda activate medm
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```


## :rocket: Getting Started

If you are confused about some parameters during usage, please refer to [Parameter Interpretation](docs/param_interpretation.md).

### 1. Train a general medical LVLM from scratch

``` bash
# For 2D medical LVLMs
# 1. pre-train (annotation format: docs/example_2d_pretrain.json)
bash scripts/train/MedM-VL-2D/pretrain_en.sh
# 2. fine-tune (annotation format: docs/example_2d_finetune.json)
bash scripts/train/MedM-VL-2D/finetune_en.sh

# For 3D medical LVLMs
# 1. pre-train (annotation format: docs/example_3d_pretrain.json)
bash scripts/train/MedM-VL-CT-Chest/pretrain_en.sh
# 2. fine-tune (annotation format: docs/example_3d_finetune.json)
bash scripts/train/MedM-VL-CT-Chest/finetune_en.sh

# In fact, there is no difference in the annotation file format between
# pre-training and fine-tuning. The former is from image-text pairs
# while the latter refers to instruction tuning data.
```

### 2. Fine-tune a specialized medical LVLM with pre-trained weights

``` bash
# For 2D medical LVLMs
# 1. download weights from Hugging Face
pip install -U huggingface_hub
huggingface-cli download --resume-download shiym2000/MedM-VL-2D-3B-en --local-dir work_dirs/MedM-VL-2D-3B-en
# 2. fine-tune using LoRA (annotation format: docs/example_2d_finetune.json)
bash scripts/train/finetune_2d.sh

# For 3D medical LVLMs
# 1. download weights from Hugging Face
pip install -U huggingface_hub
huggingface-cli download --resume-download shiym2000/MedM-VL-CT-Chest-3B-en --local-dir work_dirs/MedM-VL-CT-Chest-3B-en
# 2. fine-tune using LoRA (annotation format: docs/example_3d_finetune.json)
bash scripts/train/finetune_3d.sh

# You can choose full or LoRA fine-tuning based on available GPU memory.
```

### 3. Inference

``` bash
# For 2D medical LVLMs
# inference (annotation format: docs/example_2d_inference.json)
bash scripts/eval/inference_2d.sh

# For 3D medical LVLMs
# inference (annotation format: docs/example_3d_inference.json)
bash scripts/eval/inference_3d.sh

# Compared to `finetune.json``, `conversations` in `inference.json` lacks
# the final response, which will be generated by the model.
```

### 4. Demo

``` bash
# Launch a Gradio demo locally.
bash scripts/playground.sh
```


## :robot: Model Zoo

<table>
  <tr align="center">
    <td><b>Encoder</b></td>
    <td><b>Connector</b></td>
    <td><b>LLM</b></td>
  </tr>
  <tr valign="top">
    <td>
      <li><a href="https://arxiv.org/abs/2103.00020"> CLIP (2021) </a></li>
      <li><a href="https://arxiv.org/abs/2303.15343"> SigLIP (2023) </a></li>
      <li><a href="https://arxiv.org/abs/2404.00578"> M3D-CLIP (2023) </a></li>
      <li><a href="https://huggingface.co/collections/shiym2000/medm-clip-67f7afd8a3dbcff656466805"> MedM-CLIP <a></li>
    </td>
    <td>
      <li> MLP </li>
      <li> Spatial Pooling </li>
      <li> Attention Pooling </li>
    </td>
    <td>
      <li><a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/"> Phi-2 (2023) </a></li>
      <li><a href="https://arxiv.org/abs/2404.14219"> Phi-3 (2024) </a></li>
      <li><a href="https://arxiv.org/abs/2412.15115"> Qwen2.5 (2024) </a></li>
      <li><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"> Llama-3.2 (2024) </a></li>
    </td>
  </tr>
</table>


## :book: Citation

``` bibtex
@article{shi2025medm,
  title={MedM-VL: What Makes a Good Medical LVLM?},
  author={Shi, Yiming and Yang, Shaoshuai and Zhu, Xun and Wang, Haoyu and Li, Miao and Wu, Ji},
  journal={arXiv preprint arXiv:2504.04323},
  year={2025}
}
```


## :heart: Acknowledgements

We would like to express our gratitude to the following resources:
+ [**TinyLLaVA_Factory**](https://github.com/TinyLLaVA/TinyLLaVA_Factory) - An open-source modular codebase for small-scale large multimodal models (LMMs).
